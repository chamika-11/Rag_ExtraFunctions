{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain\n",
    "langchain-community\n",
    "chromadb\n",
    "tiktoken\n",
    "bs4\n",
    "langchain-groq \n",
    "sentence-transformers\n",
    "python-dotenv\n",
    "streamlit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd12d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3508e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd412d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"USER_AGENT\"] = \"ai-llm-agentic-rag/1.0\"\n",
    "\n",
    "# Create memory directory\n",
    "MEMORY_DIR = Path(\"./chat_memory\")\n",
    "MEMORY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"AI/LLM Expert Chat Bot with Memory\",\n",
    "    page_icon=\"ðŸ¤–\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bce32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        text-align: center;\n",
    "        padding: 1rem 0;\n",
    "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .chat-message {\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 1rem;\n",
    "        border-left: 4px solid #667eea;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #f0f2f6;\n",
    "        border-left-color: #667eea;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #e8f4fd;\n",
    "        border-left-color: #1f77b4;\n",
    "    }\n",
    "    .sidebar-content {\n",
    "        background-color: #f8f9fa;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .memory-info {\n",
    "        background-color: #e8f5e8;\n",
    "        padding: 0.5rem;\n",
    "        border-radius: 5px;\n",
    "        border-left: 3px solid #28a745;\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI and LLM focused URLs\n",
    "AI_LLM_URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\",\n",
    "    \"https://en.wikipedia.org/wiki/GPT-3\",\n",
    "    \"https://en.wikipedia.org/wiki/ChatGPT\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "    \"https://en.wikipedia.org/wiki/Deep_learning\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://en.wikipedia.org/wiki/BERT_(language_model)\",\n",
    "    \"https://en.wikipedia.org/wiki/Attention_(machine_learning)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMemoryManager:\n",
    "    \"\"\"Manages chat memory with persistence options\"\"\"\n",
    "    \n",
    "    def __init__(self, session_id=None, memory_type=\"buffer\", max_token_limit=2000):\n",
    "        self.session_id = session_id or f\"session_{int(time.time())}\"\n",
    "        self.memory_type = memory_type\n",
    "        self.max_token_limit = max_token_limit\n",
    "        self.memory_file = MEMORY_DIR / f\"{self.session_id}_memory.pkl\"\n",
    "        self.chat_history_file = MEMORY_DIR / f\"{self.session_id}_history.json\"\n",
    "        \n",
    "        # Initialize memory based on type\n",
    "        if memory_type == \"summary\":\n",
    "            self.memory = ConversationSummaryBufferMemory(\n",
    "                llm=st.session_state.get('llm'),\n",
    "                max_token_limit=max_token_limit,\n",
    "                return_messages=True,\n",
    "                memory_key=\"chat_history\"\n",
    "            )\n",
    "        else:\n",
    "            self.memory = ConversationBufferMemory(\n",
    "                return_messages=True,\n",
    "                memory_key=\"chat_history\"\n",
    "            )\n",
    "        \n",
    "        # Load existing memory if available\n",
    "        self.load_memory()\n",
    "    \n",
    "    def save_memory(self):\n",
    "        \"\"\"Save memory to file\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file, 'wb') as f:\n",
    "                pickle.dump(self.memory.chat_memory.messages, f)\n",
    "            \n",
    "            # Also save as JSON for human readability\n",
    "            chat_data = {\n",
    "                \"session_id\": self.session_id,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"type\": type(msg).__name__,\n",
    "                        \"content\": msg.content,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    for msg in self.memory.chat_memory.messages\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            with open(self.chat_history_file, 'w') as f:\n",
    "                json.dump(chat_data, f, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error saving memory: {str(e)}\")\n",
    "    \n",
    "    def load_memory(self):\n",
    "        \"\"\"Load memory from file\"\"\"\n",
    "        try:\n",
    "            if self.memory_file.exists():\n",
    "                with open(self.memory_file, 'rb') as f:\n",
    "                    messages = pickle.load(f)\n",
    "                    for msg in messages:\n",
    "                        self.memory.chat_memory.add_message(msg)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load previous memory: {str(e)}\")\n",
    "    \n",
    "    def add_message(self, human_message, ai_message):\n",
    "        \"\"\"Add a conversation turn to memory\"\"\"\n",
    "        self.memory.chat_memory.add_user_message(human_message)\n",
    "        self.memory.chat_memory.add_ai_message(ai_message)\n",
    "        self.save_memory()\n",
    "    \n",
    "    def get_memory_summary(self):\n",
    "        \"\"\"Get a summary of the current memory\"\"\"\n",
    "        messages = self.memory.chat_memory.messages\n",
    "        if not messages:\n",
    "            return \"No conversation history\"\n",
    "        \n",
    "        total_messages = len(messages)\n",
    "        human_messages = len([m for m in messages if isinstance(m, HumanMessage)])\n",
    "        ai_messages = len([m for m in messages if isinstance(m, AIMessage)])\n",
    "        \n",
    "        return f\"Memory: {total_messages} total messages ({human_messages} human, {ai_messages} AI)\"\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear all memory\"\"\"\n",
    "        self.memory.clear()\n",
    "        if self.memory_file.exists():\n",
    "            self.memory_file.unlink()\n",
    "        if self.chat_history_file.exists():\n",
    "            self.chat_history_file.unlink()\n",
    "    \n",
    "    def get_context_for_agent(self):\n",
    "        \"\"\"Get formatted context for the agent\"\"\"\n",
    "        if not self.memory.chat_memory.messages:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get recent conversation context\n",
    "        recent_messages = self.memory.chat_memory.messages[-6:]  # Last 3 turns\n",
    "        context = \"Recent conversation context:\\n\"\n",
    "        \n",
    "        for msg in recent_messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                context += f\"Human: {msg.content}\\n\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                context += f\"Assistant: {msg.content}\\n\"\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def initialize_llm():\n",
    "    \"\"\"Initialize the LLM with caching\"\"\"\n",
    "    return ChatGroq(\n",
    "        model_name=\"llama3-70b-8192\",\n",
    "        temperature=0.6,\n",
    "        max_tokens=250,\n",
    "        stop=[\"Human:\", \"Observation\"],\n",
    "    )\n",
    "\n",
    "@st.cache_resource\n",
    "def create_ai_knowledge_base(urls, persist_directory=\"./ai_llm_chroma_db\"):\n",
    "    \"\"\"Create an AI/LLM focused knowledge base from web URLs with caching\"\"\"\n",
    "    try:\n",
    "        # Load documents from web URLs\n",
    "        loader = WebBaseLoader(urls)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        return vectorstore, len(splits)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error creating knowledge base: {str(e)}\")\n",
    "        return None, 0\n",
    "\n",
    "@st.cache_resource\n",
    "def load_existing_ai_knowledge_base(persist_directory=\"./ai_llm_chroma_db\"):\n",
    "    \"\"\"Load an existing AI/LLM knowledge base with caching\"\"\"\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading knowledge base: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_retriever(vectorstore, search_type=\"similarity\", k=5):\n",
    "    \"\"\"Setup retriever with different search strategies\"\"\"\n",
    "    if search_type == \"similarity\":\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k}\n",
    "        )\n",
    "    elif search_type == \"mmr\":\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": k, \"fetch_k\": 20}\n",
    "        )\n",
    "    elif search_type == \"similarity_score\":\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"score_threshold\": 0.5, \"k\": k}\n",
    "        )\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e151be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for tools\n",
    "retriever = None\n",
    "llm = None\n",
    "memory_manager = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a298188",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_ai_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant AI/LLM documents from the knowledge base for a given query.\"\"\"\n",
    "    try:\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        if not docs:\n",
    "            return \"No relevant AI/LLM documents found for the query.\"\n",
    "        \n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            content = doc.page_content[:800] + \"...\" if len(doc.page_content) > 800 else doc.page_content\n",
    "            formatted_docs.append(f\"Document {i} (Source: {source}):\\n{content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_docs)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving documents: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def answer_ai_question_with_memory(query: str) -> str:\n",
    "    \"\"\"Answer AI/LLM related questions using retrieved context and conversation memory.\"\"\"\n",
    "    try:\n",
    "        # Get relevant documents\n",
    "        context = retrieve_ai_documents(query)\n",
    "        \n",
    "        # Get conversation memory context\n",
    "        memory_context = \"\"\n",
    "        if memory_manager:\n",
    "            memory_context = memory_manager.get_context_for_agent()\n",
    "        \n",
    "        answer_prompt = f\"\"\"\n",
    "Based on the following context about AI and Large Language Models, and considering our previous conversation, please provide a comprehensive and accurate answer to the question.\n",
    "\n",
    "{memory_context}\n",
    "\n",
    "Knowledge Base Context:\n",
    "{context}\n",
    "\n",
    "Current Question: {query}\n",
    "\n",
    "Please provide a detailed answer that:\n",
    "1. Considers our previous conversation when relevant\n",
    "2. Uses information from the provided knowledge base context\n",
    "3. Explains technical concepts clearly\n",
    "4. Mentions specific AI/LLM technologies when relevant\n",
    "5. References previous topics we discussed if they relate to the current question\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(answer_prompt)\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error answering question: {str(e)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
